{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "advProp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFBRn7u1N6iT0f9hzR/dss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66592f5f071e404782957e89bc6e1c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2533b1df0494173aef3a3956eaa1d81",
              "IPY_MODEL_19eed3d1f6f542a9aaee12df0134467d",
              "IPY_MODEL_f36ec9fa30a143c9a85ac29c494bffcf"
            ],
            "layout": "IPY_MODEL_c99ca71c07894604949096f9b759b427"
          }
        },
        "a2533b1df0494173aef3a3956eaa1d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a1e16dfc5948cf844b719bc5fb8721",
            "placeholder": "​",
            "style": "IPY_MODEL_d1fadb8642ee41668b16214d89f926bb",
            "value": ""
          }
        },
        "19eed3d1f6f542a9aaee12df0134467d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd714b4af884dd69413e8d917b0bcb5",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90cef4f427e94ec995763f97380891aa",
            "value": 170498071
          }
        },
        "f36ec9fa30a143c9a85ac29c494bffcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd015cf3c9ba43d28a87a17801167ec0",
            "placeholder": "​",
            "style": "IPY_MODEL_b22bacfbca694ec88c596c2d7d56d289",
            "value": " 170499072/? [00:02&lt;00:00, 54419570.44it/s]"
          }
        },
        "c99ca71c07894604949096f9b759b427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a1e16dfc5948cf844b719bc5fb8721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1fadb8642ee41668b16214d89f926bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fd714b4af884dd69413e8d917b0bcb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cef4f427e94ec995763f97380891aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd015cf3c9ba43d28a87a17801167ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b22bacfbca694ec88c596c2d7d56d289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justinqbui/mini-advProp/blob/main/advProp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is pytorch re-implementation of the paper [Adversarial Examples Improve Image Recognition](https://arxiv.org/pdf/1911.09665.pdf)."
      ],
      "metadata": {
        "id": "6FU1YbJFVeNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "ozn06Qjssk-q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "jW-TCN4f0u62"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "rY_E8dkzzSJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our model, we'll define and use a simple ResNet-18, but modified with 2 separate BN layers, in order to accomodate 1 used for natural training, and the other for adversarial traning. We'll start with a ResNet stem, from [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187). In the advProp paper, the authors use three auxilary BNs, one for natural, one for autoAugmented images, and one for adversarially generated images. For simplicity, we won't autoaugment any examples and stick to only two BNs, one for natural and one for adversarially generated examples.\n"
      ],
      "metadata": {
        "id": "B2OqN6uaYNGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvStem(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution ->Batch Norm -> GELU\n",
        "    We make sure to use auxilary BN because we want the learnable parameters (gamma & beta)\n",
        "    for the BN layers to be different for natural and adversarially generated examples\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernal_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernal_size, stride, padding)\n",
        "        self.clean_bn = nn.BatchNorm2d(out_channels)\n",
        "        self.adv_bn = nn.BatchNorm2d(out_channels)\n",
        "        self.GELU = nn.GELU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    \n",
        "    def forward(self, x, adv = False):\n",
        "        \"\"\"adv -> if the input is an adversially generated example\"\"\"\n",
        "        x = self.conv(x)\n",
        "        if adv:\n",
        "            x = self.adv_bn(x)\n",
        "        else:\n",
        "            x = self.clean_bn(x)\n",
        "        x = self.GELU(x)\n",
        "        x = self.maxpool(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hbus2I3i203d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now defined our stem, next we define our ResNet blocks, which make up the body of our CNN. We again use an auxilary BN and we choose GELU as our non-linearity as suggested by [Smooth Adversarial Training](https://arxiv.org/pdf/2006.14536.pdf?ref=https://githubhelp.com), which tells as that GELU or other similar smooth activation functions make our models much more robust.\n"
      ],
      "metadata": {
        "id": "xueJm0TyvWvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    conv-> BN -> GELU -> conv -> BN -> GELU\n",
        "    We use identity_downsample when our input space dimensions change\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, identity_downsample = None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.clean_bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.adv_bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.clean_bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.adv_bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.GELU = nn.GELU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "    \n",
        "    def forward(self, x, adv = False):\n",
        "        identity = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        if adv:\n",
        "            x = self.adv_bn1(x)\n",
        "        else:\n",
        "            x = self.clean_bn1(x)\n",
        "\n",
        "        x = self.GELU(x)\n",
        "        x = self.conv2(x)\n",
        "        if adv:\n",
        "            x = self.adv_bn2(x)\n",
        "        else:\n",
        "            x = self.clean_bn2(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        # residual connection\n",
        "        x += identity\n",
        "        x = self.GELU(x)\n",
        "        return x\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "tDIE-KC6CgaS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an identity downsample on convolutions that have stride of 2."
      ],
      "metadata": {
        "id": "EjumFb5ZwLvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IdentityDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride)\n",
        "        self.clean_bn = nn.BatchNorm2d(out_channels)\n",
        "        self.adv_bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x, adv = False):\n",
        "        x = self.conv(x)\n",
        "        if adv:\n",
        "            x = self.adv_bn(x)\n",
        "        else:\n",
        "            x = self.clean_bn(x)\n",
        "        return x\n",
        "    "
      ],
      "metadata": {
        "id": "KQGvTMftuAE2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MakeLayers(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom module used to make the blocks for the ResNet\n",
        "    necessary to differentiate clean vs adversarial\n",
        "    Params\n",
        "    \"\"\"\n",
        "    def __init__(self, num_blocks, in_channels, intermediate_channels, stride):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        identity_downsample = IdentityDownsample(self.in_channels, intermediate_channels, stride)\n",
        "        # here we do the downsample at the beginning of every block\n",
        "        self.layers.append(Block(self.in_channels, intermediate_channels, stride, identity_downsample))\n",
        "        self.in_channels = intermediate_channels\n",
        "        \n",
        "        for i in range(num_blocks - 1):\n",
        "            self.layers.append(Block(self.in_channels, intermediate_channels))\n",
        "\n",
        "    \n",
        "    def forward(self, x, adv = False):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, adv)\n",
        "        return x\n",
        "\n",
        "    def in_channels(self):\n",
        "        return self.in_channels"
      ],
      "metadata": {
        "id": "FZ0OYZFH1lH5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    \"\"\"\n",
        "    simple class that builds out a resnet-18 with a special stem\n",
        "    Params:\n",
        "    n_classes -> the number of classes\n",
        "    in_channels -> number of input channels (3 for (R,G, B), 1 for B,W)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes, in_channels = 3):\n",
        "        super().__init__()\n",
        "        # block size for in/out channels as the network gets deeper\n",
        "        self.stem = ConvStem(in_channels, 64, 7, 2, 3)\n",
        "        # we perform a downsample on the first convolution of layers that have stride == 2\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.layer1 = MakeLayers(2, self.in_channels, intermediate_channels = 64, stride = 1)\n",
        "        self.in_channels = self.layer1.in_channels\n",
        "\n",
        "        self.layer2 = MakeLayers(2, self.in_channels, intermediate_channels = 128, stride = 2)\n",
        "        self.in_channels = self.layer2.in_channels\n",
        "\n",
        "        self.layer3 = MakeLayers(2, self.in_channels, intermediate_channels = 256, stride = 2)\n",
        "        self.in_channels = self.layer3.in_channels\n",
        "\n",
        "        self.layer4 = MakeLayers(2, self.in_channels, intermediate_channels = 512, stride = 2)\n",
        "        self.in_channels = self.layer4.in_channels\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, n_classes)\n",
        "\n",
        "\n",
        "    \n",
        "    def forward(self, x, adv = False):\n",
        "        x = self.stem(x,adv)\n",
        "\n",
        "        x = self.layer1(x, adv)\n",
        "        x = self.layer2(x, adv)\n",
        "        x = self.layer3(x, adv)\n",
        "        x = self.layer4(x, adv)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7V_IUJCw-LAo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model = ResNet18(1000)\n",
        "    y = model(torch.randn(4, 3, 224, 224))\n",
        "    print(y.size())\n",
        "\n",
        "test()\n"
      ],
      "metadata": {
        "id": "AAEQpiMsyQrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9302eeb-fe03-45cc-dc61-bb144ce2287a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load in CIFAR10 dataset. "
      ],
      "metadata": {
        "id": "pHZOPq5Jyoww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "N96Z9QXNylv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "66592f5f071e404782957e89bc6e1c33",
            "a2533b1df0494173aef3a3956eaa1d81",
            "19eed3d1f6f542a9aaee12df0134467d",
            "f36ec9fa30a143c9a85ac29c494bffcf",
            "c99ca71c07894604949096f9b759b427",
            "d7a1e16dfc5948cf844b719bc5fb8721",
            "d1fadb8642ee41668b16214d89f926bb",
            "3fd714b4af884dd69413e8d917b0bcb5",
            "90cef4f427e94ec995763f97380891aa",
            "fd015cf3c9ba43d28a87a17801167ec0",
            "b22bacfbca694ec88c596c2d7d56d289"
          ]
        },
        "outputId": "d74975e0-011c-43bb-9868-5b92b3f6363e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66592f5f071e404782957e89bc6e1c33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def epoch(model, data_loader, opt = None):\n",
        "    \"\"\"\n",
        "    Performs a standard epoch, with no data augmentation\n",
        "    or adversarial training\n",
        "    Params:\n",
        "    model -> NN model\n",
        "    data_loader -> torch.utils.Dataloader\n",
        "    opt -> set to None if evaluating model on test set\n",
        "    \"\"\"\n",
        "    total_loss, total_err = 0., 0. \n",
        "    for x, y in data_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "        loss = nn.CrossEntropyLoss()(y_pred, y)\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        total_err += (y_pred.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * x.shape[0]\n",
        "    return total_err / len(data_loader.dataset), total_loss / len(data_loader.dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "EA5nV9EFuHka"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(10)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "S7WbirDRyG-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optim.SGD(model.parameters(), lr=.1, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
        "for i in range(1,51):\n",
        "    train_err, train_loss = epoch(model, trainloader, opt)\n",
        "    test_err, test_loss = epoch(model, testloader)\n",
        "\n",
        "    if (i % 2 == 0):\n",
        "        print(\"{:2}\".format(i), end = \"  \")\n",
        "        print(*(\"{:.6f}\".format(j) for j in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "metadata": {
        "id": "_ZcYkcROx8ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca356b7-e494-4d9b-c2d5-427fcef10fb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2  0.457340\t1.267034\t0.407500\t1.141461\n",
            " 4  0.314320\t0.887641\t0.311800\t0.885461\n",
            " 6  0.225040\t0.642420\t0.281800\t0.810661\n",
            " 8  0.161340\t0.459199\t0.266300\t0.850253\n",
            "10  0.109900\t0.310121\t0.262000\t0.909521\n",
            "12  0.071640\t0.204877\t0.261000\t1.058157\n",
            "14  0.049200\t0.140955\t0.263000\t1.176733\n",
            "16  0.034960\t0.103427\t0.254700\t1.189586\n",
            "18  0.025140\t0.074273\t0.259600\t1.503075\n",
            "20  0.017660\t0.053157\t0.256900\t1.522674\n",
            "22  0.015860\t0.045469\t0.260300\t1.462262\n",
            "24  0.015040\t0.043997\t0.259900\t1.495456\n",
            "26  0.010640\t0.031356\t0.250200\t1.770264\n",
            "28  0.010580\t0.029678\t0.257400\t1.660864\n",
            "30  0.009280\t0.027252\t0.252300\t1.687404\n",
            "32  0.007400\t0.021710\t0.253100\t1.845033\n",
            "34  0.007380\t0.022244\t0.251500\t1.712113\n",
            "36  0.005740\t0.016448\t0.257400\t1.860261\n",
            "38  0.005340\t0.016021\t0.250300\t1.837050\n",
            "40  0.005520\t0.015997\t0.252400\t1.948214\n",
            "42  0.005460\t0.016620\t0.253700\t1.780878\n",
            "44  0.003200\t0.009034\t0.248600\t1.994145\n",
            "46  0.003480\t0.011155\t0.249800\t1.961073\n",
            "48  0.004680\t0.013547\t0.252100\t2.027856\n",
            "50  0.004520\t0.014421\t0.247900\t1.824296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define fast gradient sign method (FGSM) as $adv_x = x + \\epsilon * sgn(\\Delta_xL(x,y,\\theta))$."
      ],
      "metadata": {
        "id": "pcIWw_e-O0gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm(model, x, y, eps = 0.1):\n",
        "    \"\"\" \n",
        "    performs fast gradient sign method\n",
        "    Params:\n",
        "    model -> NN model\n",
        "    x -> input image (tensor) \n",
        "    y -> corresponding label(s) for X\n",
        "    eps = epsilon hyperparameter\n",
        "    \"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    loss = nn.CrossEntropyLoss()(model(x + delta), y)\n",
        "    loss.backward()\n",
        "    return eps * delta.grad.detach().sign()"
      ],
      "metadata": {
        "id": "6frDz9FGNjhA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define projected gradient descent as $x^{t+1} = \\Pi_{x+S}(x^t+\\alpha $sgn$(\\nabla_x L(x,y,\\theta)))$. Even though it's called projected gradient descent, we're actually taking a step in the direction of the gradient, to move away from the local minima instead of toward."
      ],
      "metadata": {
        "id": "G8BB6CyuNnEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_linf(model, x, y, eps=1, alpha=1, num_iter=2, randomize=False):\n",
        "    \"\"\" \n",
        "    performs PGD in L_inf space\n",
        "    Params:\n",
        "    model -> NN model\n",
        "    x -> input image (tensor) \n",
        "    y -> corresponding label(s) for X\n",
        "    eps = epsilon hyperparameter\n",
        "    alpha = alpha hyperparameter\n",
        "    num_iters = number of iterations performed of PGD\n",
        "    randomize = random restarts (helps avoid local optima that PGD can find if started at zero point)\n",
        "    \"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(x, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * eps - eps\n",
        "    else:\n",
        "        delta = torch.zeros_like(x, requires_grad=True)\n",
        "        \n",
        "    for t in range(num_iter):\n",
        "        loss = nn.CrossEntropyLoss()(model(x + delta), y)\n",
        "        loss.backward()\n",
        "        #clamp data between [-epsilion, epsilon]\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-eps,eps)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()"
      ],
      "metadata": {
        "id": "XGqY8nrGNloV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_model = ResNet18(10)\n",
        "adv_model.to(device)"
      ],
      "metadata": {
        "id": "VrRJ-csQSjna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advProp algorithm is quite simple\n",
        "```\n",
        "for each epoch:\n",
        "    x, y = clean image mini-batch, clean image label(s)\n",
        "    delta = compute noise\n",
        "    compute adv_loss of adversarial example\n",
        "    compute clean_loss of clean example\n",
        "    minimize the loss w.r.t. the min(adv_loss + clean_loss)\n",
        "```"
      ],
      "metadata": {
        "id": "8MRvYlurT6o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def advProp_epoch(model, data_loader, attack, opt = None):\n",
        "    \"\"\"\n",
        "    implements advProp algorithm\n",
        "    \"\"\"\n",
        "    total_clean_err, total_clean_loss = 0., 0.\n",
        "    total_adv_err, total_adv_loss = 0., 0.\n",
        "    for x, y in data_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        delta = attack(model, x, y)\n",
        "        # grab prediction of adversarial example\n",
        "        y_pred_adv = model(x + delta, adv = True)\n",
        "        adv_loss = nn.CrossEntropyLoss()(y_pred_adv, y)\n",
        "        y_pred_clean = model(x)\n",
        "        clean_loss = nn.CrossEntropyLoss()(y_pred_clean, y)\n",
        "        # compute the min loss of clean + adv\n",
        "        loss = (clean_loss + adv_loss) / 2\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        \n",
        "        total_clean_err += (y_pred_clean.max(dim=1)[1] != y).sum().item()\n",
        "        total_adv_err += (y_pred_adv.max(dim=1)[1] != y).sum().item()\n",
        "        total_clean_loss += clean_loss.item() * x.shape[0]\n",
        "        total_adv_loss += adv_loss.item() * x.shape[0]\n",
        "\n",
        "    return (total_clean_err / len(data_loader.dataset),\n",
        "            total_adv_err / len(data_loader.dataset),\n",
        "            total_clean_loss / len(data_loader.dataset),\n",
        "            total_adv_loss / len(data_loader.dataset))"
      ],
      "metadata": {
        "id": "sLS5u17sSovw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optim.SGD(adv_model.parameters(), lr=.1, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
        "for i in range(1,51):\n",
        "    clean_err, adv_err, clean_loss, adv_loss = advProp_epoch(adv_model, trainloader, pgd_linf, opt)\n",
        "    test_err, test_loss = epoch(adv_model, testloader)\n",
        "\n",
        "    if (i % 2 == 0):\n",
        "        print(\"{:2}\".format(i), end = \"  \")\n",
        "        print(*(\"{:.6f}\".format(j) for j in (clean_err, clean_loss, adv_err, adv_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "metadata": {
        "id": "xYvy9c2sV787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1a46d5-e747-4fb7-a210-438d7532973b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2  0.500740\t1.405459\t0.468760\t1.345284\t0.432800\t1.262209\n",
            " 4  0.325180\t0.925962\t0.207780\t0.614954\t0.326100\t0.932992\n",
            " 6  0.244000\t0.695638\t0.160400\t0.479563\t0.280800\t0.819140\n",
            " 8  0.182240\t0.520222\t0.173180\t0.513462\t0.257400\t0.783842\n",
            "10  0.135560\t0.383466\t0.207660\t0.610962\t0.271500\t0.895839\n",
            "12  0.097180\t0.272971\t0.241460\t0.708779\t0.259000\t0.947442\n",
            "14  0.068700\t0.193662\t0.260400\t0.764145\t0.260700\t1.011175\n",
            "16  0.050580\t0.143295\t0.265880\t0.783320\t0.257100\t1.170417\n",
            "18  0.035840\t0.103049\t0.265540\t0.774524\t0.259600\t1.265479\n",
            "20  0.028540\t0.084470\t0.268300\t0.784799\t0.255400\t1.356969\n",
            "22  0.025640\t0.073284\t0.269040\t0.783438\t0.255700\t1.399395\n",
            "24  0.021220\t0.059996\t0.270180\t0.792809\t0.257700\t1.510917\n",
            "26  0.016400\t0.049188\t0.267880\t0.777332\t0.250700\t1.463987\n",
            "28  0.015160\t0.044483\t0.258680\t0.757370\t0.251600\t1.548814\n",
            "30  0.014220\t0.040806\t0.258900\t0.756449\t0.255200\t1.599421\n",
            "32  0.013660\t0.038880\t0.256200\t0.744126\t0.252600\t1.571965\n",
            "34  0.012740\t0.036826\t0.252080\t0.744096\t0.259500\t1.629597\n",
            "36  0.011240\t0.032657\t0.253900\t0.739582\t0.254800\t1.648439\n",
            "38  0.010820\t0.030548\t0.246000\t0.723182\t0.255200\t1.684531\n",
            "40  0.009120\t0.026810\t0.251320\t0.728835\t0.255900\t1.728374\n",
            "42  0.008980\t0.025557\t0.247480\t0.718609\t0.248600\t1.710523\n",
            "44  0.007740\t0.023120\t0.241400\t0.707317\t0.252800\t1.713626\n",
            "46  0.008260\t0.025053\t0.242700\t0.709469\t0.253500\t1.728632\n",
            "48  0.007020\t0.021003\t0.240360\t0.697250\t0.253900\t1.666210\n",
            "50  0.008320\t0.024140\t0.235160\t0.688422\t0.254400\t1.756051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that after training, both the adversarially trained model and the one only trained on clean images perform about the same. In the paper, only ResNet50+ larger variants were used. If we were to train both models for more epochs, I'd hypothesize that the adversarially trained model would begin to outperform the clean model, because we'd have more \"training data\" so to speak by generating new examples and having less issues with overfitting. We also used the optimizer proposed in the ResNet, instead of the one proposed in advProp (RMSProp).\n"
      ],
      "metadata": {
        "id": "Hbh4fe6yUYHy"
      }
    }
  ]
}